{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aa338d-8caa-4ec9-aa7f-90da99a01a5b",
   "metadata": {},
   "source": [
    "### AUTHOR: Aayush Joshi\n",
    "### TITLE: Guide to Data Processing with Dense Basis Using an Automated Program\n",
    "### Last verified: April 2024\n",
    "### Contact author: Aayush Joshi and Louise Edwards\n",
    "\n",
    "The objective of this notebook is to employ an automated program to analyze the Truth Summary and Object Catalog from DP0 DC2. The primary focus is to query proximate, luminous galaxy clusters and ascertain their star formation history.\n",
    "\n",
    "The link below provides documentation for Dense Basis installation, dependencies and other features. Proceed with this notebook once the installation is completed:\n",
    "https://dense-basis.readthedocs.io/en/latest/\n",
    "\n",
    "This notebook incorporates elements from tutorial notebooks 1-8 and is structured intachieve three main purpose:\n",
    "\n",
    "1. **Program Utilization:** This section provides a comprehensive guide on how to effectively use the automated program. It includes step-by-step instructions and best practices for optimal results.\n",
    "2. **Data Processing:** Here, you will find detailed procedures on how to process data from the Truth Summary and Object Catalog. This includes data cleaning, transformation, and preparation steps to ensure the data is ready for analysis.\n",
    "3. **Data Visualization:** The final section focuses on visualizing the processed data. It demonstrates how to generate meaningful plots that can aid in understanding the star formation history of galaxy clusters.\n",
    "\n",
    "By following this notebook, users will gain a thorough understanding of how to study galaxy clusters using the DP0 DC2 dataset, from initial data processing to final visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c4f0a-b263-423f-b33a-48f2e422c1ae",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df567f9-cd8c-4d15-9592-f38995e79c83",
   "metadata": {},
   "source": [
    "This document serves as a comprehensive guide to understanding the functionalities of two key Jupyter notebooks: `DB_Processor.ipynb` and `DB_Visualizer.ipynb`. These notebooks play a crucial role in the processing and visualization of large datasets.\r\n",
    "\r\n",
    "The `DB_Processor.ipynb` notebook is designed to handle the processing of extensive datasetsIt is an automated script programmed in Python that k adopts a strategy of dividing the dataset into smaller, more manageable subsets. This division facilitates more efficient data processing and reduces the computational load. Once processed, each subset is saved in local directories. This approach allows for the retrieval of specific subsets as and when required, thereby optimizing the use of storage resources and improving the overall efficiency of data handling.\r\n",
    "\r\n",
    "On the other hand, the `DB_Visualizer.ipynb` notebook is tasked with the visualization of the processed data. The ability to visualize data while it is still being processed is a significant advantage. It allows for preliminary analysis and insights to be drawn even before the completion of the entire data processing task. The `DB_Visualizer.ipynb` retrieves the processed subsets of data and generates informative plots from them. These visualizations can aid in identifying patterns, trends, and anomalies in the data, providing valuable insights that can guide further data processing and analy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6fb15-a190-445d-94cc-711d318f4821",
   "metadata": {},
   "source": [
    "# Directory Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed72b9-f3c5-4978-94cb-4ae185998d2b",
   "metadata": {},
   "source": [
    "The program employs directories to manage data:\r\n",
    "\r\n",
    "- **pregrids**: Houses the processed atlas.\r\n",
    "- **query**: Retains the DP0 queried data.\r\n",
    "- **DV_Dataframe**: Stores the processed data.\r\n",
    "\r\n",
    "Key input parameters, such as the redshift range (z) and the number of lookback time parameters (Nparam), are utilized to identify each object within these directories. It's crucial to refer to the notes in `DV_Processor.ipynb` to ensure the program is functioning correctly.\r\n",
    "\r\n",
    "The sub-directories and files adhere to a standard structure: `\"z_'z_min'_'z_max'Nparam'Nparam'\"`, where:\r\n",
    "\r\n",
    "- `z_min`: Lower limit of redshift\r\n",
    "- `z_max`: Upper limit of redshift\r\n",
    "- `Nparam`: Number of lookback time parameterse parametersata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd2fc3-2145-4817-958e-2e4a22904981",
   "metadata": {},
   "source": [
    "The notebooks are structured to offer users the flexibility to proceed with any segment of the program post-initialization. However, this could lead to unforeseen outcomes, particularly in `DB_Processor.ipynb`. Therefore, it's advisable to bypass sections only if you're confident in doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1154ed8-0de0-429c-bd58-cc28cf81ac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
